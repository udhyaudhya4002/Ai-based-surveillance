{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7312675,"sourceType":"datasetVersion","datasetId":2151340},{"sourceId":11449765,"sourceType":"datasetVersion","datasetId":7173619},{"sourceId":11450169,"sourceType":"datasetVersion","datasetId":7173912},{"sourceId":342984,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":286897,"modelId":307710}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ğŸ“¦ Step 1: Install Dependencies\n\n!pip install opencv-python-headless tensorflow keras --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T12:58:46.073754Z","iopub.execute_input":"2025-04-17T12:58:46.073968Z","iopub.status.idle":"2025-04-17T12:58:51.279085Z","shell.execute_reply.started":"2025-04-17T12:58:46.073947Z","shell.execute_reply":"2025-04-17T12:58:51.277879Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ğŸ“ Step 2: Extract Frames from .avi Videos\nimport cv2\nimport os\n\n# This function extracts frames from .avi files inside class folders\n# and saves them in a corresponding output folder structure\n\ndef extract_frames_from_videos(video_dir, output_dir, frame_rate=10):\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    for class_folder in os.listdir(video_dir):\n        class_path = os.path.join(video_dir, class_folder)\n        output_class_path = os.path.join(output_dir, class_folder)\n        os.makedirs(output_class_path, exist_ok=True)\n\n        for video_name in os.listdir(class_path):\n            if not video_name.endswith(\".avi\"):\n                continue\n\n            video_path = os.path.join(class_path, video_name)\n            cap = cv2.VideoCapture(video_path)\n\n            count = 0\n            saved = 0\n            while cap.isOpened():\n                ret, frame = cap.read()\n                if not ret:\n                    break\n                if count % frame_rate == 0:\n                    frame_file = f\"{os.path.splitext(video_name)[0]}_{saved}.jpg\"\n                    frame_path = os.path.join(output_class_path, frame_file)\n                    cv2.imwrite(frame_path, frame)\n                    saved += 1\n                count += 1\n\n            cap.release()\n\n# Extract frames from AVI videos in Train and Test directories\nextract_frames_from_videos(\"/kaggle/input/smartcity-cctv-violence-detection-dataset-scvd/SCVD/SCVD_converted/Train\", \"/kaggle/working/frames/train\")\nextract_frames_from_videos(\"/kaggle/input/smartcity-cctv-violence-detection-dataset-scvd/SCVD/SCVD_converted/Test\", \"/kaggle/working/frames/test\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T17:24:56.768496Z","iopub.execute_input":"2025-04-17T17:24:56.768797Z","iopub.status.idle":"2025-04-17T17:27:00.433758Z","shell.execute_reply.started":"2025-04-17T17:24:56.768767Z","shell.execute_reply":"2025-04-17T17:27:00.432972Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ğŸ“Š Step 3: Load Image Data for Training\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nimg_height, img_width = 224, 224\nbatch_size = 32\n\ntrain_dir = \"/kaggle/working/frames/train\"\ntest_dir = \"/kaggle/working/frames/test\"\n\ndatagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = datagen.flow_from_directory(\n    train_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical'\n)\n\ntest_generator = datagen.flow_from_directory(\n    test_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical'\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T17:27:00.434496Z","iopub.execute_input":"2025-04-17T17:27:00.434749Z","iopub.status.idle":"2025-04-17T17:27:12.691064Z","shell.execute_reply.started":"2025-04-17T17:27:00.434730Z","shell.execute_reply":"2025-04-17T17:27:12.690303Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ğŸ§  Step 4: Create a Simple CNN Model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n\nmodel = Sequential([\n    Conv2D(32, (3,3), activation='relu', input_shape=(img_height, img_width, 3)),\n    MaxPooling2D(2,2),\n    Conv2D(64, (3,3), activation='relu'),\n    MaxPooling2D(2,2),\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.5),\n    Dense(3, activation='softmax')  # 3 classes: Normal, Violence, Weaponized\n])\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T17:27:12.692609Z","iopub.execute_input":"2025-04-17T17:27:12.693203Z","iopub.status.idle":"2025-04-17T17:27:15.100491Z","shell.execute_reply.started":"2025-04-17T17:27:12.693174Z","shell.execute_reply":"2025-04-17T17:27:15.099688Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ğŸ’¾ Step 5: Define Callbacks (Checkpoint + Learning Rate Scheduler)\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n\ncheckpoint_cb = ModelCheckpoint(\n    \"/kaggle/working/best_model.keras\",\n    save_best_only=True,\n    monitor='val_accuracy',\n    verbose=1\n)\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1)\n\ncallbacks = [checkpoint_cb, reduce_lr]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T17:27:15.101651Z","iopub.execute_input":"2025-04-17T17:27:15.101882Z","iopub.status.idle":"2025-04-17T17:27:15.107623Z","shell.execute_reply.started":"2025-04-17T17:27:15.101863Z","shell.execute_reply":"2025-04-17T17:27:15.106979Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ğŸš€ Step 6: Train the Model\nEPOCHS = 10\n\nhistory = model.fit(\n    train_generator,\n    epochs=EPOCHS,\n    validation_data=test_generator,\n    callbacks=callbacks\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T17:27:15.110702Z","iopub.execute_input":"2025-04-17T17:27:15.110957Z","execution_failed":"2025-04-17T17:27:58.957Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the full model (architecture + weights)\nmodel.save(\"/kaggle/working/final_violence_model1.h5\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T13:47:17.516973Z","iopub.execute_input":"2025-04-17T13:47:17.517276Z","iopub.status.idle":"2025-04-17T13:47:18.245312Z","shell.execute_reply.started":"2025-04-17T13:47:17.517255Z","shell.execute_reply":"2025-04-17T13:47:18.244647Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing.image import img_to_array\nimport os\n\n# Load the trained model\nmodel = load_model(\"/kaggle/working/final_violence_model.keras\")  # adjust path if needed\n\n# Class labels (adjust if your labels are different)\nclass_names = ['Normal', 'Violence', 'Weapon']  # or ['NonViolent', 'Violent'] if using 2-class\n\n# Function to predict on a video\ndef predict_video(video_path, img_height=224, img_width=224, frame_skip=10):\n    cap = cv2.VideoCapture(video_path)\n    predictions = []\n\n    frame_count = 0\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n        \n        if frame_count % frame_skip == 0:\n            resized_frame = cv2.resize(frame, (img_width, img_height))\n            img = img_to_array(resized_frame) / 255.0\n            img = np.expand_dims(img, axis=0)\n\n            pred = model.predict(img)\n            predicted_class = np.argmax(pred)\n            predictions.append(predicted_class)\n\n        frame_count += 1\n    \n    cap.release()\n\n    # Majority voting\n    if not predictions:\n        return \"No frames were processed.\"\n    \n    final_class = max(set(predictions), key=predictions.count)\n    return f\"Prediction: {class_names[final_class]}\"\n\n# ğŸ” Test with your uploaded video\ntest_video_path = \"/kaggle/input/your-test-folder/sample.avi\"  # ğŸ‘ˆ Update this path\n\nresult = predict_video(test_video_path)\nprint(result)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T17:28:16.886004Z","iopub.execute_input":"2025-04-17T17:28:16.886312Z","iopub.status.idle":"2025-04-17T17:28:20.099670Z","shell.execute_reply.started":"2025-04-17T17:28:16.886285Z","shell.execute_reply":"2025-04-17T17:28:20.098427Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing.image import img_to_array\nimport os\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T05:02:10.566044Z","iopub.execute_input":"2025-04-18T05:02:10.566376Z","iopub.status.idle":"2025-04-18T05:02:10.570723Z","shell.execute_reply.started":"2025-04-18T05:02:10.566348Z","shell.execute_reply":"2025-04-18T05:02:10.569767Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"model = load_model('/kaggle/input/violence-detection/tensorflow2/default/1/final_violence_model1.h5')  # Replace with your actual path\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T05:02:10.999482Z","iopub.execute_input":"2025-04-18T05:02:10.999797Z","iopub.status.idle":"2025-04-18T05:02:11.859535Z","shell.execute_reply.started":"2025-04-18T05:02:10.999768Z","shell.execute_reply":"2025-04-18T05:02:11.858818Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"IMG_SIZE = 224  # Adjust if your model uses different size\n\ndef preprocess_frame(frame):\n    frame = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n    frame = frame.astype(\"float32\") / 255.0\n    frame = img_to_array(frame)\n    frame = np.expand_dims(frame, axis=0)\n    return frame\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T05:02:11.860770Z","iopub.execute_input":"2025-04-18T05:02:11.861061Z","iopub.status.idle":"2025-04-18T05:02:11.865577Z","shell.execute_reply.started":"2025-04-18T05:02:11.861035Z","shell.execute_reply":"2025-04-18T05:02:11.864686Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"def predict_video(video_path, frame_skip=30):\n    cap = cv2.VideoCapture(video_path)\n    predictions = []\n\n    frame_count = 0\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n        if frame_count % frame_skip == 0:\n            processed = preprocess_frame(frame)\n            pred = model.predict(processed)[0]\n            predictions.append(pred)\n        frame_count += 1\n    cap.release()\n\n    # Average prediction across frames\n    if len(predictions) == 0:\n        return \"No frames processed\"\n\n    avg_prediction = np.mean(predictions, axis=0)\n    class_idx = np.argmax(avg_prediction)\n    labels = [\"Normal\", \"Violence\", \"Weapon\"]  # Modify as per your modelâ€™s output\n\n    return labels[class_idx]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T05:02:13.707473Z","iopub.execute_input":"2025-04-18T05:02:13.707757Z","iopub.status.idle":"2025-04-18T05:02:13.713397Z","shell.execute_reply.started":"2025-04-18T05:02:13.707735Z","shell.execute_reply":"2025-04-18T05:02:13.712355Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# video_path = \"/kaggle/input/smartcity-cctv-violence-detection-dataset-scvd/SCVD/SCVD_converted_sec_split/Test/Weaponized/Weaponized-Test030.avi\"  \n# video_path = \"/kaggle/input/smartcity-cctv-violence-detection-dataset-scvd/SCVD/SCVD_converted_sec_split/Test/Normal/Normal-Test002.avi\" \nvideo_path = \"/kaggle/input/smartcity-cctv-violence-detection-dataset-scvd/SCVD/SCVD_converted_sec_split/Test/Violence/Violence-Test009.avi\" # Make sure you uploaded this\nresult = predict_video(video_path)\nprint(\"ğŸ¥ Prediction for the video:\", result)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T05:08:08.763982Z","iopub.execute_input":"2025-04-18T05:08:08.764314Z","iopub.status.idle":"2025-04-18T05:08:08.891441Z","shell.execute_reply.started":"2025-04-18T05:08:08.764249Z","shell.execute_reply":"2025-04-18T05:08:08.890544Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\nğŸ¥ Prediction for the video: Violence\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"import cv2\n\ndef convert_to_avi(input_path, output_path):\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        print(\"âŒ Error opening video file.\")\n        return\n\n    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))  \n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) \n    fps    = cap.get(cv2.CAP_PROP_FPS)\n\n    fourcc = cv2.VideoWriter_fourcc(*'XVID')  # Use XVID codec for AVI\n    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n        out.write(frame)\n\n    cap.release()\n    out.release()\n    print(f\"âœ… Converted video saved as: {output_path}\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-17T17:27:58.959Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_video = \"/kaggle/input/your-video-folder/your_video.mp4\"  # your original video\noutput_video = \"/kaggle/input/test-set/Villupuram School and college boy _Fight  _college _fight _trending _status _shorts _chennai(360P).mp4\"             # avi output\n\nconvert_to_avi(input_video, output_video)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-17T17:27:58.959Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result = predict_video(output_video)\nprint(\"ğŸ¯ Prediction:\", result)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-17T17:27:58.959Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T19:08:59.742337Z","iopub.execute_input":"2025-04-17T19:08:59.742635Z","iopub.status.idle":"2025-04-17T19:08:59.763236Z","shell.execute_reply.started":"2025-04-17T19:08:59.742613Z","shell.execute_reply":"2025-04-17T19:08:59.762571Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import keras\nprint(keras.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T19:09:39.111484Z","iopub.execute_input":"2025-04-17T19:09:39.111782Z","iopub.status.idle":"2025-04-17T19:09:39.116298Z","shell.execute_reply.started":"2025-04-17T19:09:39.111759Z","shell.execute_reply":"2025-04-17T19:09:39.115417Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\nfrom keras.models import load_model\nfrom keras.preprocessing.image import img_to_array\n\n# === CONFIG ===\nIMG_SIZE = 224  # Adjust based on your model\nLABELS = [\"Normal\", \"Violence\", \"Weapon\"]  # Adjust according to your model output\nFRAME_SKIP = 30  # Process every 30th frame\n\n# === Load Model ===\nmodel_path = \"/kaggle/input/violence-detection/tensorflow2/default/1/final_violence_model1.h5\"  # <- Update this\nmodel = load_model(model_path, compile=False)\n\n# === Frame Preprocessing ===\ndef preprocess_frame(frame):\n    frame = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n    frame = frame.astype(\"float32\") / 255.0\n    frame = img_to_array(frame)\n    frame = np.expand_dims(frame, axis=0)\n    return frame\n\n# === Predict + Save New Video ===\ndef predict_and_save_video(input_video_path, output_video_path):\n    cap = cv2.VideoCapture(input_video_path)\n\n    if not cap.isOpened():\n        print(\"âŒ Error: Cannot open input video.\")\n        return\n\n    # Get video properties\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n\n    # Define video writer\n    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n\n    frame_count = 0\n    current_label = \"Detecting...\"\n\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        if frame_count % FRAME_SKIP == 0:\n            processed = preprocess_frame(frame)\n            pred = model.predict(processed)[0]\n            label_idx = np.argmax(pred)\n            current_label = LABELS[label_idx]\n\n        # Overlay prediction on frame\n        cv2.putText(frame, f\"Prediction: {current_label}\", (10, 30),\n                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n\n        out.write(frame)\n        frame_count += 1\n\n    cap.release()\n    out.release()\n    print(\"âœ… Prediction video saved:\", output_video_path)\n\n# === File Paths ===\ninput_video_path = \"/kaggle/input/test-set/Villupuram School and college boy _Fight  _college _fight _trending _status _shorts _chennai(360P).mp4\"\noutput_video_path = \"predicted_output.avi\"\n\npredict_and_save_video(input_video_path, output_video_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T20:28:31.300334Z","iopub.execute_input":"2025-04-17T20:28:31.300605Z","iopub.status.idle":"2025-04-17T20:29:01.868738Z","shell.execute_reply.started":"2025-04-17T20:28:31.300583Z","shell.execute_reply":"2025-04-17T20:29:01.867787Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import Video\nVideo(\"predicted_output.avi\", embed=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T20:30:53.689382Z","iopub.execute_input":"2025-04-17T20:30:53.689734Z","iopub.status.idle":"2025-04-17T20:30:54.516444Z","shell.execute_reply.started":"2025-04-17T20:30:53.689706Z","shell.execute_reply":"2025-04-17T20:30:54.515049Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom tensorflow.keras.models import load_model\n\n# âœ… Load your .h5 model\nmodel_path = \"/kaggle/input/violence-detection/tensorflow2/default/1/final_violence_model1.h5\"\nmodel = load_model(model_path)\n\n# âœ… Parameters\nimg_size = (224, 224)\ntest_dir = \"/kaggle/input/smartcity-cctv-violence-detection-dataset-scvd/SCVD/SCVD_converted/Test\"  # Change to your test path\nclass_labels = ['Normal', 'Violence', 'Weaponized']  # Adjust based on your folders\n\ndef preprocess_frame(frame):\n    frame = cv2.resize(frame, img_size)\n    frame = frame.astype(\"float32\") / 255.0\n    frame = img_to_array(frame)\n    return frame\n\ny_true = []\ny_pred = []\n\nfor class_name in os.listdir(test_dir):\n    class_path = os.path.join(test_dir, class_name)\n    for video_file in os.listdir(class_path):\n        if not video_file.endswith('.avi'):\n            continue\n        video_path = os.path.join(class_path, video_file)\n        cap = cv2.VideoCapture(video_path)\n\n        frames = []\n        count = 0\n        while cap.isOpened() and count < 20:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            frames.append(preprocess_frame(frame))\n            count += 1\n        cap.release()\n\n        if not frames:\n            continue\n\n        frames = np.array(frames)\n        preds = model.predict(frames)\n        avg_pred = np.mean(preds, axis=0)\n        pred_class = np.argmax(avg_pred)\n\n        y_true.append(class_labels.index(class_name))\n        y_pred.append(pred_class)\n\n# âœ… Accuracy\nacc = accuracy_score(y_true, y_pred)\nprint(f\"âœ… Average accuracy on test videos: {acc*100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T05:26:14.892680Z","iopub.execute_input":"2025-04-18T05:26:14.893003Z","iopub.status.idle":"2025-04-18T05:26:26.255291Z","shell.execute_reply.started":"2025-04-18T05:26:14.892976Z","shell.execute_reply":"2025-04-18T05:26:26.254522Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\nâœ… Average accuracy on test videos: 37.80%\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing.image import img_to_array\n\n# === Load the model ===\nmodel_path = \"/kaggle/input/violence-detection/tensorflow2/default/1/final_violence_model1.h5\"\nmodel = load_model(model_path)\n\n# === Parameters ===\nimg_size = (224, 224)\ntest_dir = \"/kaggle/input/smartcity-cctv-violence-detection-dataset-scvd/SCVD/SCVD_converted/Test\"  # âœ… Change this to your real path\nclass_labels = ['Normal', 'Violence', 'Weaponized']   # âœ… Make sure this matches your folders\n\ndef preprocess_frame(frame):\n    frame = cv2.resize(frame, img_size)\n    frame = frame.astype(\"float32\") / 255.0\n    frame = img_to_array(frame)\n    return frame\n\n# === Accuracy counters ===\nclass_correct = {label: 0 for label in class_labels}\nclass_total = {label: 0 for label in class_labels}\n\n# === Loop through each class ===\nfor class_name in class_labels:\n    class_path = os.path.join(test_dir, class_name)\n    \n    for video_file in os.listdir(class_path):\n        if not video_file.endswith(\".avi\"):\n            continue\n        \n        video_path = os.path.join(class_path, video_file)\n        cap = cv2.VideoCapture(video_path)\n\n        frames = []\n        count = 0\n        while cap.isOpened() and count < 20:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            frames.append(preprocess_frame(frame))\n            count += 1\n        cap.release()\n\n        if not frames:\n            continue\n\n        frames = np.array(frames)\n        preds = model.predict(frames)\n        avg_pred = np.mean(preds, axis=0)\n        pred_class_idx = np.argmax(avg_pred)\n\n        true_idx = class_labels.index(class_name)\n\n        # === Accuracy counting ===\n        class_total[class_name] += 1\n        if pred_class_idx == true_idx:\n            class_correct[class_name] += 1\n\n# === Print accuracy per class ===\nprint(\"\\nâœ… Accuracy per class:\")\nfor label in class_labels:\n    correct = class_correct[label]\n    total = class_total[label]\n    acc = (correct / total) * 100 if total > 0 else 0\n    print(f\"{label}: {acc:.2f}% ({correct}/{total})\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T05:50:04.491634Z","iopub.execute_input":"2025-04-18T05:50:04.491863Z","iopub.status.idle":"2025-04-18T05:50:34.041375Z","shell.execute_reply.started":"2025-04-18T05:50:04.491840Z","shell.execute_reply":"2025-04-18T05:50:34.040444Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\nâœ… Accuracy per class:\nNormal: 17.39% (8/46)\nViolence: 75.00% (9/12)\nWeaponized: 58.33% (14/24)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}